{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad559d8",
   "metadata": {},
   "source": [
    "### 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668d246",
   "metadata": {},
   "source": [
    "![Tokenizer](https://stanford-cs336.github.io/spring2025-lectures/images/tokenized-example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5ac52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "class Tokenizer(ABC):\n",
    "  def encode(self, string: str):\n",
    "    raise NotImplementedError\n",
    "  def decode(self, indices: list[int]):\n",
    "    raise NotImplementedError\n",
    "\n",
    "def get_compression_ratio(string: str, indices: list[int]) -> float:\n",
    "  num_bytes = len(string.encode(\"utf-8\"))\n",
    "  num_token = len(indices)\n",
    "  return num_bytes / num_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e7889",
   "metadata": {},
   "source": [
    "#### Character Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdaecb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 44, 32, 127757, 33, 32, 20320, 22909, 33]\n",
      "Hello, 游깴! 擔먼봏!\n",
      "1.5384615384615385\n"
     ]
    }
   ],
   "source": [
    "class CharacterTokenizer(Tokenizer):\n",
    "  def encode(self, string: str):\n",
    "    return list(map(ord, string))\n",
    "  def decode(self, indices: list[int]):\n",
    "    return ''.join(map(chr, indices))\n",
    "\n",
    "tokenizer = CharacterTokenizer()\n",
    "string = \"Hello, 游깴! 擔먼봏!\"\n",
    "indices = tokenizer.encode(string)\n",
    "reconstructed_string = tokenizer.decode(indices)\n",
    "\n",
    "print(indices)\n",
    "print(reconstructed_string)\n",
    "print(get_compression_ratio(string, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975419d0",
   "metadata": {},
   "source": [
    "- **Problem 1**: this is a very large vocabulary.\n",
    "- **Problem 2**: many characters are quite rare (e.g., 游깴), which is inefficient use of the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c77b7f",
   "metadata": {},
   "source": [
    "#### Bytes Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77c4630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72, 101, 108, 108, 111, 44, 32, 240, 159, 140, 141, 33, 32, 228, 189, 160, 229, 165, 189, 33]\n",
      "Hello, 游깴! 擔먼봏!\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "class BytesTokenizer(Tokenizer):\n",
    "  def encode(self, string: str):\n",
    "    string_bytes = string.encode(\"utf-8\")\n",
    "    return list(map(int, string_bytes))\n",
    "  def decode(self, indices: list[int]):\n",
    "    string_bytes = bytes(indices)\n",
    "    return string_bytes.decode(\"utf-8\")\n",
    "\n",
    "tokenizer = BytesTokenizer()\n",
    "string = \"Hello, 游깴! 擔먼봏!\"\n",
    "indices = tokenizer.encode(string)\n",
    "reconstructed_string = tokenizer.decode(indices)\n",
    "\n",
    "print(indices)\n",
    "print(reconstructed_string)\n",
    "print(get_compression_ratio(string, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b5a702",
   "metadata": {},
   "source": [
    "While **byte-level tokenization** can alleviate the **out-of-vocabulary** issues faced by word-level tokenizers, tokenizing text into bytes results in extremely **long input sequences**. This slows down model training, since a  5 sentence with 10 words might only be 10 tokens long in a word-level language model, but could be 50 or more tokens long in a character-level model (depending on the length of the words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e273938",
   "metadata": {},
   "source": [
    "#### Subword Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264cbcfc",
   "metadata": {},
   "source": [
    "### regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e809294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "def remove_special_tokens(text: str, specials_tokens:list[str], keep_specials_tokens=True) -> list[str]:\n",
    "  if specials_tokens is None:\n",
    "    return [text]\n",
    "\n",
    "  specials_tokens = sorted(specials_tokens, key=len, reverse=True)\n",
    "  delimiter = \"|\".join(re.escape(token) for token in specials_tokens)\n",
    "  if not keep_specials_tokens:\n",
    "    chunks = re.split(delimiter, text)\n",
    "  else: \n",
    "    chunks = re.split(f\"({delimiter})\", text)\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43390c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '<BOS>', 'Hello ', '<BOS>', '', '<EOS>', ' World', '<EOS>', ' ']\n"
     ]
    }
   ],
   "source": [
    "text = \" <BOS>Hello <BOS><EOS> World<EOS> \"\n",
    "tokens = [\"<BOS>\", \"<EOS>\"]\n",
    "result = remove_special_tokens(text, tokens)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EECS_498",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
