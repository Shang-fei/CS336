# Assignment 1: Basics

This assignment covers the fundamentals of building language models from scratch.

## Topics Covered

1. **Tokenization**
   - Byte-Pair Encoding (BPE)
   - Training tokenizers on text data
   - Encoding and decoding text

2. **Language Model Architecture**
   - Transformer architecture
   - Multi-head attention
   - Feed-forward networks
   - Layer normalization

3. **Training**
   - Training loop implementation
   - Loss computation
   - Gradient descent and optimization
   - Learning rate scheduling

4. **Text Generation**
   - Sampling strategies
   - Temperature scaling
   - Top-k and top-p sampling

## Files

(Implementation files will be added here)

## Setup

```bash
# Install dependencies
pip install torch numpy transformers

# Run tests
pytest tests/
```

## Progress

- [ ] Part 1: BPE Tokenization
- [ ] Part 2: Tokenizer Implementation
- [ ] Part 3: Language Model Architecture
- [ ] Part 4: Training Requirements
- [ ] Part 5: Training Loop
- [ ] Part 6: Text Generation
- [ ] Part 7: Experiments

## Notes

This assignment provides the foundation for understanding how modern language models work by building one from scratch.
